![ft comp](https://s3.sa-east-1.amazonaws.com/remotar-assets-prod/company-profile-covers/cl7god9gt00lx04wg4p2a93zt.jpg)

## üìï Cursos e Conte√∫dos
Aqui est√° uma vis√£o geral dos cursos e conte√∫dos que explorei at√© agora:

### ‚òÅÔ∏è Learn By Example: Hadoop, MapReduce for Big Data problems

- O curso 'Learn By Example: Hadoop, MapReduce for Big Data problems' me ensinou a processar grandes volumes de dados usando Hadoop e MapReduce. Atrav√©s de exemplos pr√°ticos, aprendi a escrever programas MapReduce, configurar clusters Hadoop e analisar dados n√£o estruturados. Agora, estou equipado para lidar com desafios de Big Data e extrair informa√ß√µes valiosas de conjuntos de dados em larga escala.

#### üéâ Certificado 

![Alt text](image.png)

### ‚òÅÔ∏è Forma√ß√£o Spark com Pyspark : o Curso Completo


- O curso completo 'Forma√ß√£o Spark com Pyspark' oferece uma ampla compreens√£o do processamento de dados em escala com o uso do Apache Spark e Pyspark. Atrav√©s de aulas pr√°ticas e te√≥ricas, aprendi a manipular, analisar e extrair insights valiosos de grandes conjuntos de dados, bem como a criar aplica√ß√µes de Big Data. Este curso √© essencial para aprimorar minhas habilidades em an√°lise de dados e me preparar para desafios no mundo real.

#### üéâ Certificado

![Alt text](image-1.png)


# Data & Analytics - PB - AWS 7/10

## SE√á√ÉO 3: Exercicios

[Tarefa: Python com Pandas e Numpy](Tarefa1)

[Tarefa: Apache Spark - Contador de Palavras](secao3/Tarefa2)


## SE√á√ÉO 4: Laborat√©rio AWS

### Criando e executando um novo job no AWS Glue

![Alt text](image-2.png)
Criando um job

![Alt text](image-3.png)
Executando um job



## SE√á√ÉO 5: Desafio - Parte I

[Tarefa 3: Desafio Parte 1 - ETL](secao5)

![Alt text](image-8.png)
Criando o bucket para armazenar os csv

![![Alt text](image-12.png)](image-11.png)
Criando um usu√°rio IAM

![![Alt text](image-14.png)](image-13.png)
Criando um grupo de usu√°rio 

![Alt text](<Captura de tela 2023-11-10 132243.png>)
build do dockerfile para executar o desafio.py

![Alt text](image-9.png)
Aqui a exporta√ß√£o dos dados no bucket

![Alt text](image-10.png)
Resultado ap√≥s rodar o container com a imagem docker criada


## ‚ú® Conclus√£o
O curso de Hadoop com MapReduce aborda o processamento de Big Data com foco no ecossistema Hadoop, enquanto o curso de Spark com Pyspark se concentra no Apache Spark. Ambos s√£o cursos essenciais para profissionais que desejam dominar ferramentas importantes no mundo da an√°lise de dados em larga escala. 
